{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SHARNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPihOMJkvTyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQfcIix3qhCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrueGELU(nn.Module):#PyTorch defines GELU as x*sigmoid(x), \n",
        "  def forward(self,x):\n",
        "    return x*torch.sigmoid(1.702*x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09LvxYk_sYlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "  if type(m)==nn.Linear:\n",
        "    m.weight.data.normal_(mean=0.0,std=0.1/np.sqrt(self.ninp))\n",
        "    m.bias.data.fill_(0.0)\n",
        "  if type(m)==nn.Embedding:\n",
        "    m.weight.data.normal_(mean=0.0,std=0.1/np.sqrt(self.ninp))\n",
        "  if type(m)==nn.LayerNorm:\n",
        "    m.weight.data.normal_(mean=0.0,std=0.1/np.sqrt(self.ninp))\n",
        "    m.bias.data.fill_(0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXpVbZ9zvyST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Boom(nn.Module):\n",
        "  def __init__(self,input_dim,blowup_dim=2048,dropout=0.1,shortcut=False):\n",
        "    super(Boom,self).__init__()\n",
        "    self.lin1=nn.Linear(input_dim,blowup_dim,bias=True)\n",
        "    self.dropout=nn.Dropout(dropout) if dropout else None\n",
        "    #self.true_gelu=TrueGELU()\n",
        "    if shortcut==False:\n",
        "      self.lin2=nn.Linear(blowup_dim,input_im)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x_dim=x.shape[-1]\n",
        "    x=nn.GELU(self.lin1(x)) \n",
        "    #x=self.true_gelu(x)\n",
        "    x=self.dropout(X)\n",
        "    if shortcut==False:\n",
        "      x=self.lin2(x)\n",
        "    else:\n",
        "      x=torch.narrow(x,-1,0,x.shape[-1]//x_dim*x_dim) #trimming some stuff to make it possible to reshape into (...,x.shape[-1]//x_dim,x_dim)\n",
        "      x=x.view(*x.shape[:-1],x.shape[-1]//x_dim,x_dim) #breaking the tensor x into equal chunks of size along the innermost dimesnion.\n",
        "                                                       #index slicing can do the job too, but this is much faster, and achieves the same thing.\n",
        "      x=x.sum(dim=-2)                                  #summing up these chunks\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzrpOazG7Qjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#class SimplifiedAttention(nn.Module):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpNYN6f17ydM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DefaultAttention(nn.Module):\n",
        "  def __init__(self,nhidden,q=True,k=False,v=False,num_heads=1,dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.mha=nn.MultiHeadedAttention(embed_dim=nhid,num_heads=num_heads,dropout=dropout)\n",
        "\n",
        "  def forward(self,q,k,v,attn_mask=None):\n",
        "    return self.mha(q,k,v,attn_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0oXVq8UrVox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self,embed_dim,hidden_dim,num_heads=1,dropout=0.0,residual=True,simpified_attention=False):\n",
        "    super().__init__()\n",
        "    self.attention=None\n",
        "    if simplified_attention==False:\n",
        "      self.attention=DefaultAttention(embed_dim,num_heads=num_heads,dropout=dropout)\n",
        "    #else:\n",
        "    #  self.attention=SimplifiedAttention()\n",
        "    self.boom=Boom(embed_dim,hidden_dim,dropout=dropout)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.gelu=nn.TrueGELU()\n",
        "    self.residual=residual\n",
        "    self.lstm=nn.LSTM(input_size=embed_dim,hidden_size=embed_dim)\n",
        "\n",
        "    self.norm0=nn.LayerNorm(embed_dim,eps=1e-12) #must init multiple layer norm\n",
        "    self.norm1=nn.LayerNorm(embed_dim,eps=1e-12) #modules because layer norm \n",
        "    self.norm2=nn.LayerNorm(embed_dim,eps=1e-12) #consists of trainable params\n",
        "    self.norm3=nn.LayerNorm(embed_dim,eps=1e-12)\n",
        "    self.norm4=nn.LayerNorm(embed_dim,eps=1e-12)\n",
        "\n",
        "  def forward(self,h,pos_encoding,attn_mask,mem=None,hidden=None):\n",
        "    h=self.norm0(h)\n",
        "    x,new_hidden=self.lstm(h)\n",
        "    h_dim=h.shape[-1]\n",
        "    z=torch.narrow(x,-1,0,x.shape[-1]//h_dim*h_dim)\n",
        "    z=z.view(*x.shape[:-1],x.shape[-1]//dim_h,dim_h)\n",
        "    x=self.dropout(z).sum(dim=-2)\n",
        "    if self.residual==True: #skip connection\n",
        "      h=h+x\n",
        "    else:\n",
        "      h=x.float()\n",
        "    \n",
        "    attention_weights=None\n",
        "    new_mem=[]\n",
        "    h=self.norm1(h)\n",
        "    mh=self.norm2(h)\n",
        "    if mem is not None:\n",
        "      k=torch.cat([mem,mh],dim=0)\n",
        "    else:\n",
        "      k=mh\n",
        "    new_mem=k[-len(pos_encoding):] #positional encoding adds information about the relative position of tokens in the sequence of input data\n",
        "    x,attention_weights=self.attention(q=h,k=k,v=k,attn_mask=attn_mask)\n",
        "    x=self.dropout(x)\n",
        "    h=x+h\n",
        "\n",
        "    h,x=self.norm3(h),self.norm4(x)\n",
        "    x=self.boom(x)\n",
        "    x=self.drop(x)\n",
        "    h=x+h\n",
        "\n",
        "    return h,new_mem,new_hidden,attention_weights\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6ijQ5VA5zBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SHARNN(nn.Module):\n",
        "  def __init__(self,num_embeddings,embed_dim,hidden_dim,num_layers,dropout=0.5)\n",
        "  super().__init__()\n",
        "  self.embed_dim=embed_dim\n",
        "  self.hidden_dim=hidden_dim\n",
        "  self.num_layers=num_layers\n",
        "  self.num_embeddings=num_embeddings\n",
        "  self.max_positions=5000\n",
        "  self.num_heads=1\n",
        "  self.dropout=nn.Dropout(dropout)\n",
        "  \n",
        "  self.block_list=nn.ModuleList() #functions just like a Python list, but to store any number of nn modules. Helps if the number of layers are passed as input\n",
        "  for i in range(num_layers):\n",
        "    block=Block(self.embed_dim,self.hidden_dim,num_heads=1,dropout=dropout,residual=False)\n",
        "    self.block_list.append(block)\n",
        "  self.positional_embedding=[0]*self.max_positions #usually the fastest way to initialise single valued lists\n",
        "  self.encoder=nn.Embedding(num_embeddings,embed_dim)\n",
        "  self.decoder = nn.Linear(embed_dim, num_embeddings)\n",
        "\n",
        "  def forward(self,x,hidden=None,mems=None):\n",
        "    encoding=self.encoder(x)\n",
        "    h=self.dropout(encoding)\n",
        "\n",
        "    if mems is not None:\n",
        "      mems=[m[-(self.max_positions-len(h)):] for m in mems]\n",
        "    total_length=len(x)+(len(mems[0]) if mems else 0)\n",
        "    new_hidden=[]\n",
        "    new_mems=[]\n",
        "    attn_weights=[]\n",
        "    attn_mask=torch.full((len(x),len(x)),-float('Inf'))\n",
        "    attn_mask=attn_mask.to(device)\n",
        "    attn_mask=torch.triu(attn_mask,diagonal=1)\n",
        "    if mems:\n",
        "      max_mems=max(len(m) for m in mems)\n",
        "      z=torch.zeros((len(x),max_mems),device=h.device,dtype=h.dtype)\n",
        "      attn_mask=torch.cat([z,attn_mask],dim=-1)\n",
        "\n",
        "    for i,block in enumerate(self.blocks):\n",
        "      memory=mems[i] if mems else None\n",
        "      hid=hidden[i] if hidden else None\n",
        "      h,nm,nh,weights=block(h,self.positional_encoding,attn_mask,memory,hid)\n",
        "      new_hidden.append(nh)\n",
        "      #attn_weights.append(weights)\n",
        "      new_mems.append(nm)\n",
        "    \n",
        "    h=self.dropout(h)\n",
        "\n",
        "    return h,new_hidden,new_mems"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}